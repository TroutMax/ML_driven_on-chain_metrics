{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf27c2b",
   "metadata": {},
   "source": [
    "# ML-Driven On-Chain Metrics Analysis\n",
    "\n",
    "## Package Requirements\n",
    "This notebook requires the following packages. Run the installation cell below if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Installation (run once)\n",
    "# Uncomment and run these lines if packages are missing\n",
    "\n",
    "# Core ML packages\n",
    "# !pip install scikit-learn xgboost lightgbm\n",
    "\n",
    "# Technical analysis and financial data\n",
    "# !pip install ta yfinance\n",
    "\n",
    "# Statistical analysis\n",
    "# !pip install statsmodels arch\n",
    "\n",
    "# Data processing (should already be installed)\n",
    "# !pip install pandas numpy plotly\n",
    "\n",
    "print(\"‚úÖ Required packages:\")\n",
    "print(\"- pandas, numpy (data manipulation)\")\n",
    "print(\"- scikit-learn (machine learning)\")\n",
    "print(\"- plotly (visualization)\")\n",
    "print(\"- xgboost, lightgbm (gradient boosting)\")\n",
    "print(\"- ta (technical analysis)\")\n",
    "print(\"- statsmodels (statistical modeling)\")\n",
    "print(\"- dune-client (Dune API)\")\n",
    "print(\"- python-dotenv (environment variables)\")\n",
    "\n",
    "# Verify key imports\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    print(\"\\n‚úÖ Core packages imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå Missing package: {e}\")\n",
    "    print(\"Run the pip install commands above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2bb693",
   "metadata": {},
   "source": [
    "# Fetching on-chain Data from Dune API #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b214a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from dune_client.client import DuneClient\n",
    "from dune_client.types import QueryParameter\n",
    "from dune_client.client import DuneClient\n",
    "from dune_client.query import QueryBase\n",
    "\n",
    "\n",
    "# Clear any existing environment variables first\n",
    "if 'DUNE_API_KEY' in os.environ:\n",
    "    del os.environ['DUNE_API_KEY']\n",
    "\n",
    "# Load environment variables from .env file in project root\n",
    "load_dotenv('../.env', override=True)\n",
    "\n",
    "API_KEY = os.getenv('DUNE_API_KEY')      # Make sure your .env file has DUNE_API_KEY=your_actual_api_key\n",
    "# print(f\"API_KEY: {API_KEY  }\")\n",
    "\n",
    "if API_KEY:\n",
    "    dune = DuneClient(API_KEY)\n",
    "    df = dune.get_latest_result_dataframe(5745512)  # From https://dune.com/troutmax/onchain-metrics dashboard\n",
    "    print(\"Query executed successfully!\")\n",
    "    print(\"===============================================================================\")\n",
    "    print(df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"API key not found - please check your .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94327e",
   "metadata": {},
   "source": [
    "# Importing Dune API data using new system #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a413ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Fixed to save to project root data directory\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath('..')  # Go up one level from notebooks/\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Added to Python path: {project_root}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "from src.data_providers import setup_providers\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup providers\n",
    "manager = setup_providers()\n",
    "\n",
    "# Test connections\n",
    "print(\"Connection Status:\")\n",
    "for provider, status in manager.test_all_connections().items():\n",
    "    print(f\"  {provider}: {'‚úÖ Connected' if status else '‚ùå Failed'}\")\n",
    "\n",
    "# Get Dune data using the new system\n",
    "dune = manager.get_provider('dune')\n",
    "if dune:\n",
    "    # Get data\n",
    "    df = dune.get_bot_volume_data()\n",
    "    \n",
    "    # Save raw data to PROJECT ROOT data directory (not notebooks/data)\n",
    "    data_dir = os.path.join(project_root, 'data', 'raw', 'dune')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    raw_path = os.path.join(data_dir, f'bot_volume_{timestamp}.parquet')\n",
    "    \n",
    "    # Save the data\n",
    "    df.to_parquet(raw_path)\n",
    "    print(f\"‚úÖ Raw data saved: {raw_path}\")\n",
    "    \n",
    "    # Verify file was created and show some info\n",
    "    if os.path.exists(raw_path):\n",
    "        file_size_mb = os.path.getsize(raw_path) / 1024 / 1024\n",
    "        print(f\"üìä File size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"üìÅ Full path: {raw_path}\")\n",
    "    \n",
    "    # Continue with your existing analysis\n",
    "    print(f\"\\nData shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Show data preview\n",
    "    print(f\"\\nData preview:\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dune provider not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "print(\"=====================================================================================\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d75222",
   "metadata": {},
   "source": [
    "# Current File Structure Analysis #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1131afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimal_structure():\n",
    "    \"\"\"Create optimal data directory structure\"\"\"\n",
    "    \n",
    "    # Use project root data directory, not notebooks/data\n",
    "    base_data_dir = '../data'  # Go up one level from notebooks/\n",
    "    \n",
    "    optimal_structure = {\n",
    "        f'{base_data_dir}/raw/dune/': 'Raw Dune API responses',\n",
    "        f'{base_data_dir}/raw/hyperliquid/': 'Raw Hyperliquid data',\n",
    "        f'{base_data_dir}/raw/backup/': 'Backup copies of critical datasets',\n",
    "        \n",
    "        f'{base_data_dir}/processed/daily/': 'Daily aggregated features',\n",
    "        f'{base_data_dir}/processed/hourly/': 'Hourly features for real-time signals',\n",
    "        f'{base_data_dir}/processed/features/': 'Engineered features ready for ML',\n",
    "        \n",
    "        f'{base_data_dir}/cache/': 'Temporary processing files',\n",
    "        f'{base_data_dir}/cache/api_responses/': 'Cached API responses',\n",
    "        \n",
    "        f'{base_data_dir}/models/': 'Trained ML models',\n",
    "        f'{base_data_dir}/models/checkpoints/': 'Model training checkpoints',\n",
    "        \n",
    "        f'{base_data_dir}/exports/': 'Data exports for sharing/presentation',\n",
    "        f'{base_data_dir}/metadata/': 'Data quality reports and schemas'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    print(\"üèóÔ∏è  CREATING OPTIMAL DIRECTORY STRUCTURE\\n\")\n",
    "    \n",
    "    for dir_path, description in optimal_structure.items():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"‚úÖ Created: {dir_path:<30} | {description}\")\n",
    "    \n",
    "    # Create a README for data organization\n",
    "    readme_content = \"\"\"# Data Directory Structure\n",
    "\n",
    "## Raw Data\n",
    "- `raw/dune/`: Raw blockchain data from Dune Analytics\n",
    "- `raw/hyperliquid/`: Raw DEX data from Hyperliquid\n",
    "- `raw/backup/`: Critical dataset backups\n",
    "\n",
    "## Processed Data  \n",
    "- `processed/daily/`: Daily aggregated metrics\n",
    "- `processed/hourly/`: Hourly features for real-time analysis\n",
    "- `processed/features/`: ML-ready feature datasets\n",
    "\n",
    "## Cache & Temporary\n",
    "- `cache/`: Temporary processing files\n",
    "- `cache/api_responses/`: Cached API calls (1-hour TTL)\n",
    "\n",
    "## Models & Outputs\n",
    "- `models/`: Trained ML models and scalers\n",
    "- `exports/`: Clean datasets for sharing\n",
    "- `metadata/`: Data schemas and quality reports\n",
    "\n",
    "## File Naming Convention\n",
    "- Raw: `{source}_{dataset}_{YYYYMMDD_HHMMSS}.parquet`\n",
    "- Processed: `{feature_type}_{timeframe}_{YYYYMMDD}.parquet`\n",
    "- Models: `{model_type}_{version}_{YYYYMMDD}.pkl`\n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "# Create the optimal structure\n",
    "create_optimal_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c170bc82",
   "metadata": {},
   "source": [
    "# Plotting Bot Volume Data across chains # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Convert block_date to datetime if it's not already\n",
    "df['block_date'] = pd.to_datetime(df['block_date'])\n",
    "\n",
    "# Sort by date for proper plotting\n",
    "df_sorted = df.sort_values('block_date')\n",
    "\n",
    "print(\"Data columns:\", df.columns.tolist())\n",
    "print(\"\\nUnique blockchains:\", df['blockchain'].unique())\n",
    "print(f\"\\nDate range: {df['block_date'].min()} to {df['block_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive plots with Plotly\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Volume USD by Chain', 'Number of Users by Chain', \n",
    "                   'Bot Revenue USD by Chain', 'Number of Trades by Chain'),\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Get unique blockchains and generate enough colors\n",
    "blockchains = df['blockchain'].unique()\n",
    "# Generate colors dynamically - cycle through color palettes if needed\n",
    "colors = px.colors.qualitative.Set1 + px.colors.qualitative.Set2 + px.colors.qualitative.Set3\n",
    "colors = colors[:len(blockchains)]  # Take only as many as needed\n",
    "\n",
    "print(f\"Number of blockchains: {len(blockchains)}\")\n",
    "print(f\"Blockchains: {list(blockchains)}\")\n",
    "\n",
    "# Plot 1: Volume USD by Chain\n",
    "for i, blockchain in enumerate(blockchains):\n",
    "    chain_data = df_sorted[df_sorted['blockchain'] == blockchain]\n",
    "    volume_col = pd.to_numeric(chain_data['volumeUSD'], errors='coerce')\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=chain_data['block_date'], y=volume_col,\n",
    "                  name=f'{blockchain}', line=dict(color=colors[i]),\n",
    "                  legendgroup=blockchain, showlegend=True),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Plot 2: Number of Users by Chain  \n",
    "for i, blockchain in enumerate(blockchains):\n",
    "    chain_data = df_sorted[df_sorted['blockchain'] == blockchain]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=chain_data['block_date'], y=chain_data['numberOfUsers'],\n",
    "                  name=f'{blockchain}', line=dict(color=colors[i]),\n",
    "                  legendgroup=blockchain, showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Plot 3: Bot Revenue USD by Chain\n",
    "for i, blockchain in enumerate(blockchains):\n",
    "    chain_data = df_sorted[df_sorted['blockchain'] == blockchain]\n",
    "    bot_revenue_col = pd.to_numeric(chain_data['botRevenueUSD'], errors='coerce')\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=chain_data['block_date'], y=bot_revenue_col,\n",
    "                  name=f'{blockchain}', line=dict(color=colors[i]),\n",
    "                  legendgroup=blockchain, showlegend=False),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Plot 4: Number of Trades by Chain\n",
    "for i, blockchain in enumerate(blockchains):\n",
    "    chain_data = df_sorted[df_sorted['blockchain'] == blockchain]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=chain_data['block_date'], y=chain_data['numberOfTrades'],\n",
    "                  name=f'{blockchain}', line=dict(color=colors[i]),\n",
    "                  legendgroup=blockchain, showlegend=False),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Trading Bot Activity by Blockchain Over Time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked area chart for all blockchains volume\n",
    "fig = go.Figure()\n",
    "\n",
    "# First, convert all data to numeric and calculate total volumes per blockchain\n",
    "df_numeric = df.copy()\n",
    "df_numeric['volumeUSD'] = pd.to_numeric(df_numeric['volumeUSD'], errors='coerce')\n",
    "\n",
    "# Calculate total volume per blockchain to order them\n",
    "blockchain_totals = df_numeric.groupby('blockchain')['volumeUSD'].sum().sort_values(ascending=False)\n",
    "blockchains_ordered = blockchain_totals.index.tolist()  # Highest to lowest\n",
    "\n",
    "# Define specific colors for key blockchains\n",
    "blockchain_colors = {\n",
    "    'Solana': 'rgba(138, 43, 226, 0.8)',      # Purple for Solana\n",
    "    'Ethereum': 'rgba(54, 162, 235, 0.8)',    # Blue for Ethereum  \n",
    "    'BSC': 'rgba(255, 205, 86, 0.8)',         # Yellow for BSC\n",
    "    'Base': 'rgba(75, 192, 192, 0.8)',        # Teal for Base\n",
    "    'TON': 'rgba(255, 99, 132, 0.8)',         # Red for TON\n",
    "    'Avalanche': 'rgba(153, 102, 255, 0.8)',  # Light Purple\n",
    "    'Arbitrum': 'rgba(255, 159, 64, 0.8)',    # Orange\n",
    "    'sonic': 'rgba(199, 199, 199, 0.8)',      # Gray\n",
    "    'Blast': 'rgba(83, 102, 255, 0.8)',       # Light Blue\n",
    "    'Fantom': 'rgba(255, 99, 255, 0.8)',      # Pink\n",
    "    'Polygon': 'rgba(99, 255, 132, 0.8)',     # Light Green\n",
    "    'Optimism': 'rgba(255, 132, 99, 0.8)',    # Light Red\n",
    "    'Linea': 'rgba(132, 255, 99, 0.8)',       # Green\n",
    "    'Scroll': 'rgba(255, 165, 0, 0.8)'        # Dark Orange\n",
    "}\n",
    "\n",
    "print(f\"Found {len(blockchains_ordered)} blockchains ordered by total volume:\")\n",
    "for i, blockchain in enumerate(blockchains_ordered):\n",
    "    total_vol = blockchain_totals[blockchain]\n",
    "    print(f\"{i+1}. {blockchain}: ${total_vol:,.0f}\")\n",
    "\n",
    "# Add each blockchain in order from largest to smallest (largest becomes bottom layer)\n",
    "for i, blockchain in enumerate(blockchains_ordered):\n",
    "    blockchain_data = df_sorted[df_sorted['blockchain'] == blockchain].copy()\n",
    "    \n",
    "    if len(blockchain_data) > 0:\n",
    "        # Ensure proper date sorting\n",
    "        blockchain_data = blockchain_data.sort_values('block_date').reset_index(drop=True)\n",
    "        \n",
    "        # Convert volume to numeric (it's stored as string in scientific notation)\n",
    "        blockchain_data['volumeUSD'] = pd.to_numeric(blockchain_data['volumeUSD'], errors='coerce')\n",
    "        \n",
    "        # Remove any NaN values that might have been created\n",
    "        blockchain_data = blockchain_data.dropna(subset=['volumeUSD'])\n",
    "        \n",
    "        # Get color for this blockchain\n",
    "        color = blockchain_colors.get(blockchain, f'rgba({50 + i*30}, {100 + i*20}, {150 + i*10}, 0.7)')\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=blockchain_data['block_date'], \n",
    "                y=blockchain_data['volumeUSD'],\n",
    "                name=blockchain,\n",
    "                fill='tonexty' if i > 0 else 'tozeroy',  # First one fills to zero, others stack on top\n",
    "                mode='none',  # No lines, just filled area\n",
    "                fillcolor=color,\n",
    "                hovertemplate=f'<b>{blockchain}</b><br>Date: %{{x}}<br>Volume: $%{{y:,.0f}}<extra></extra>',\n",
    "                stackgroup='one'  # Enable stacking\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Clean formatting\n",
    "fig.update_layout(\n",
    "    title='Bot Volume Last 765 Days', \n",
    "    title_x=0.5,\n",
    "    xaxis_title='Date', \n",
    "    yaxis_title='Volume USD', \n",
    "    height=600,\n",
    "    width=1200,\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    font=dict(family=\"Arial\", size=12),\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        orientation=\"v\",\n",
    "        yanchor=\"top\",\n",
    "        y=1,\n",
    "        xanchor=\"left\",\n",
    "        x=1.02\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickformat='$,.0f',\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        gridwidth=1\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgray',\n",
    "        gridwidth=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics and Trends\n",
    "print(\"=== TRADING BOT ACTIVITY SUMMARY ===\\n\")\n",
    "\n",
    "# First, convert all numeric columns from scientific notation strings to proper numbers\n",
    "df_numeric = df.copy()\n",
    "numeric_columns = ['volumeUSD', 'botRevenueUSD', 'numberOfUsers', 'numberOfTrades', 'numberOfNewUsers', 'averageVolumePerUserUSD', 'averageVolumePerTradeUSD']\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df_numeric.columns:\n",
    "        df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')\n",
    "\n",
    "# Overall statistics\n",
    "total_volume = df_numeric['volumeUSD'].sum()\n",
    "total_revenue = df_numeric['botRevenueUSD'].sum()\n",
    "total_trades = df_numeric['numberOfTrades'].sum()\n",
    "avg_users_per_day = df_numeric['numberOfUsers'].mean()\n",
    "\n",
    "print(f\"Total Volume: ${total_volume:,.2f}\")\n",
    "print(f\"Total Bot Revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"Total Trades: {total_trades:,}\")\n",
    "print(f\"Average Users per Day: {avg_users_per_day:.0f}\")\n",
    "print(f\"Revenue as % of Volume: {(total_revenue/total_volume)*100:.2f}%\\n\")\n",
    "\n",
    "# By Blockchain\n",
    "print(\"=== BY BLOCKCHAIN ===\")\n",
    "blockchain_summary = df_numeric.groupby('blockchain').agg({\n",
    "    'volumeUSD': ['sum', 'mean'],\n",
    "    'botRevenueUSD': ['sum', 'mean'],\n",
    "    'numberOfUsers': ['sum', 'mean'],\n",
    "    'numberOfTrades': ['sum', 'mean'],\n",
    "    'numberOfNewUsers': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "blockchain_summary.columns = ['Total_Volume', 'Avg_Daily_Volume', 'Total_Revenue', 'Avg_Daily_Revenue',\n",
    "                             'Total_Users', 'Avg_Daily_Users', 'Total_Trades', 'Avg_Daily_Trades', 'Total_New_Users']\n",
    "\n",
    "print(blockchain_summary)\n",
    "\n",
    "# Recent trends (last 30 days)\n",
    "recent_data = df_numeric[df_numeric['block_date'] >= df_numeric['block_date'].max() - pd.Timedelta(days=30)]\n",
    "print(f\"\\n=== RECENT TRENDS (Last 30 Days) ===\")\n",
    "print(f\"Recent Volume: ${recent_data['volumeUSD'].sum():,.2f}\")\n",
    "print(f\"Recent Revenue: ${recent_data['botRevenueUSD'].sum():,.2f}\")\n",
    "print(f\"Most Active Chain: {recent_data.groupby('blockchain')['volumeUSD'].sum().idxmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4861752",
   "metadata": {},
   "source": [
    "# Simple Sentiment Analysis Using Vader for Tweets #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "sample_tweet = \"Bitcoin is going to the moon!\"\n",
    "print(f\"Sentiment score: {analyze_sentiment(sample_tweet)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995bfa20",
   "metadata": {},
   "source": [
    "# Training a Random Forest Classifier on Features #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b309eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering and ML Example\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create features from our existing data\n",
    "def create_features(df):\n",
    "    \"\"\"Create ML features from trading bot data\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Convert to numeric\n",
    "    numeric_columns = ['volumeUSD', 'botRevenueUSD', 'numberOfUsers', 'numberOfTrades']\n",
    "    for col in numeric_columns:\n",
    "        df_features[col] = pd.to_numeric(df_features[col], errors='coerce')\n",
    "    \n",
    "    # Sort by blockchain and date for proper rolling calculations\n",
    "    df_features = df_features.sort_values(['blockchain', 'block_date'])\n",
    "    \n",
    "    # Rolling features (7-day windows)\n",
    "    df_features['volume_ma_7d'] = df_features.groupby('blockchain')['volumeUSD'].rolling(7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    df_features['volume_std_7d'] = df_features.groupby('blockchain')['volumeUSD'].rolling(7, min_periods=1).std().reset_index(0, drop=True)\n",
    "    \n",
    "    # Volume change features\n",
    "    df_features['volume_pct_change'] = df_features.groupby('blockchain')['volumeUSD'].pct_change()\n",
    "    df_features['volume_change_7d'] = df_features.groupby('blockchain')['volumeUSD'].pct_change(7)\n",
    "    \n",
    "    # User activity features\n",
    "    df_features['users_per_trade'] = df_features['numberOfUsers'] / (df_features['numberOfTrades'] + 1)\n",
    "    df_features['volume_per_user'] = df_features['volumeUSD'] / (df_features['numberOfUsers'] + 1)\n",
    "    \n",
    "    # Create target: High volume day (top 25% of volume for each blockchain)\n",
    "    df_features['volume_rank'] = df_features.groupby('blockchain')['volumeUSD'].rank(pct=True)\n",
    "    df_features['high_volume_day'] = (df_features['volume_rank'] > 0.75).astype(int)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Create features\n",
    "print(\"Creating features...\")\n",
    "df_ml = create_features(df_numeric)\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df_ml = df_ml.dropna()\n",
    "\n",
    "if len(df_ml) > 50:  # Only proceed if we have enough data\n",
    "    # Select features for ML\n",
    "    feature_cols = [\n",
    "        'volume_ma_7d', 'volume_std_7d', 'volume_pct_change', 'volume_change_7d',\n",
    "        'users_per_trade', 'volume_per_user', 'numberOfUsers', 'numberOfTrades'\n",
    "    ]\n",
    "    \n",
    "    X = df_ml[feature_cols]\n",
    "    y = df_ml['high_volume_day']\n",
    "    \n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Target distribution: {y.value_counts()}\")\n",
    "    \n",
    "    # Time series split (respecting temporal order)\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            max_depth=5, \n",
    "            random_state=42,\n",
    "            class_weight='balanced'  # Handle class imbalance\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "    \n",
    "    print(f\"\\n=== Overall Results ===\")\n",
    "    print(f\"Mean CV Accuracy: {np.mean(accuracies):.3f} (+/- {np.std(accuracies)*2:.3f})\")\n",
    "    \n",
    "    # Feature importance from last model\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== Feature Importance ===\")\n",
    "    print(feature_importance)\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough data for ML training. Need more historical data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto_finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
